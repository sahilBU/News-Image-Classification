{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \n",
    "    width, height = img.shape[0], img.shape[1]\n",
    "    \n",
    "    img = image.array_to_img(img, scale=False)\n",
    "    \n",
    "    desired_width, desired_height = 224, 224\n",
    "    \n",
    "    if width < desired_width:\n",
    "        desired_width = width\n",
    "    if height < desired_height:\n",
    "        desired_height = height\n",
    "    \n",
    "    start_x = np.maximum(0, int((width-desired_width)//2))\n",
    "    \n",
    "    img = img.crop((start_x, np.maximum(0, height-desired_height), start_x + desired_width, height))\n",
    "    \n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    img = image.img_to_array(img)\n",
    "    \n",
    "    return img / 255  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_images():\n",
    "    train_folder = \"data/news_imgs/train\"\n",
    "    \n",
    "    files = os.listdir(train_folder)\n",
    "    \n",
    "    files = list(map(lambda x: os.path.join(train_folder, x), files)) # load all the training images\n",
    "    \n",
    "    # get started with just 20 images for training\n",
    "    train_images = files[0:20]\n",
    "\n",
    "    # preprocess images and load them\n",
    "    images = []\n",
    "    for item in train_images:\n",
    "        img = image.load_img(item)\n",
    "        img = image.img_to_array(img)\n",
    "        img = preprocess(img)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_numpy_array(items): \n",
    "    values = []\n",
    "    \n",
    "    for index, row in items.iterrows():\n",
    "\n",
    "        cats = {\n",
    "                'protest': True,\n",
    "                'violence': True,\n",
    "                'sign': True,\n",
    "                'photo': True,\n",
    "                'fire': True,\n",
    "                'police': True,\n",
    "                'children': True,\n",
    "                'group_20': True,\n",
    "                'group_100': True,\n",
    "                'flag': True,\n",
    "                'night': True,\n",
    "                'shouting': True\n",
    "               }\n",
    "\n",
    "        if row['protest']:\n",
    "            \n",
    "            array = []\n",
    "            \n",
    "            for cat in cats:\n",
    "                f = float(row[cat])\n",
    "                array.append(f)\n",
    "            \n",
    "            narray = np.array(array)\n",
    "        \n",
    "            values.append(narray)\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_labels():\n",
    "    \n",
    "    labels = pd.read_csv(\"data/news_imgs/training.csv\") # load csv of text data provided\n",
    "    \n",
    "    ll = convert_labels_to_numpy_array(labels)\n",
    " \n",
    "    train_labels = ll[0:20]    # train only in 20 images first\n",
    "    \n",
    "    return train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(images, size):\n",
    "    return np.asarray(images).reshape(size,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_model):\n",
    "    \n",
    "    bottleneck_input = base_model.get_layer(index=0).input\n",
    "    bottleneck_output = base_model.get_layer(index=-2).output\n",
    "    \n",
    "    bottleneck_model = Model(inputs=bottleneck_input, outputs=bottleneck_output)\n",
    "    \n",
    "    for layer in bottleneck_model.layers:\n",
    "        \n",
    "        layer.trainable = False\n",
    "    \n",
    "    new_model = Sequential()\n",
    "    \n",
    "    new_model.add(bottleneck_model)\n",
    "    \n",
    "    new_model.add(Dense(11, input_dim=2048, activation='softmax'))\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3()\n",
    "\n",
    "new_model = get_model(base_model)\n",
    "\n",
    "new_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_training_labels()\n",
    "\n",
    "images = reshape_images(get_training_images(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.        , 0.34870572, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.15315054, 1.        , 1.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.52754146, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.18533357, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.07231162, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.27033797, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.31023582, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 1.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.51776107, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 1.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.26822416, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.19315345, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.35367946, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.40481016, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.11597707, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.       , 0.4129733, 1.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 1.       , 1.       , 0.       , 0.       , 0.       ]), array([1.        , 0.43300978, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.        , 0.27449507, 0.        , 0.        , 0.        ,\n",
      "       0.        , 1.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.       , 0.7475597, 0.       , 0.       , 1.       , 0.       ,\n",
      "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ]), array([1.        , 0.24147626, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), array([1.       , 0.3174697, 1.       , 0.       , 0.       , 0.       ,\n",
      "       0.       , 1.       , 0.       , 0.       , 0.       , 0.       ]), array([1.        , 0.29679516, 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_54 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-caa8e82b33a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_54 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "\n",
    "model.fit(images, labels[0], epochs=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = base_model.output\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# x = Dense(2048, activation='relu')(x)\n",
    "\n",
    "# # and a logistic layer -- let's say we have 10 classes\n",
    "# predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "    \n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# model.fit(fimages, train_labels,\n",
    "#                     steps_per_epoch=10000, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
