{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Sequential,load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    Reshape an image to 224 x 224 x 3, so that it can be understood by our model\n",
    "    \n",
    "    Args:\n",
    "        image: an arbitary height x width x 3 image\n",
    "    Output:\n",
    "        returns the converted image\n",
    "    \"\"\"\n",
    "    \n",
    "    width, height = img.shape[0], img.shape[1]\n",
    "    \n",
    "    img = image.array_to_img(img, scale=False)\n",
    "    \n",
    "    desired_width, desired_height = 224, 224\n",
    "    \n",
    "    if width < desired_width:\n",
    "        desired_width = width\n",
    "    if height < desired_height:\n",
    "        desired_height = height\n",
    "    \n",
    "    start_x = np.maximum(0, int((width-desired_width)//2))\n",
    "    \n",
    "    img = img.crop((start_x, np.maximum(0, height-desired_height), start_x + desired_width, height))\n",
    "    \n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    img = image.img_to_array(img)\n",
    "    \n",
    "    return img / 255  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path, correct_images, is_train=True):\n",
    "    \"\"\"\n",
    "    Get all the images\n",
    "    \n",
    "    Args:\n",
    "        path               : folder path to load images from \n",
    "        correct_images     : image names which have protest label set\n",
    "        is_train           : to identify if the images to load are training images or test images\n",
    "    Output:\n",
    "        returns an array of processed images \n",
    "    \"\"\"\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    files = list(map(lambda x: os.path.join(path, x), files)) # load all the images\n",
    "    \n",
    "    if is_train:\n",
    "        filtered_files = [image for image in files if image in correct_images]\n",
    "    else:\n",
    "        filtered_files = [image for image in files if image in correct_images]\n",
    "    \n",
    "    \n",
    "    train_images = filtered_files\n",
    "    \n",
    "    images = []\n",
    "    for item in train_images:\n",
    "        img = image.load_img(item)\n",
    "        img = image.img_to_array(img)\n",
    "        img = preprocess(img)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_numpy_array(items): \n",
    "    \"\"\"\n",
    "    Filter all labels based on the protest parameter\n",
    "    \n",
    "    Args:\n",
    "        items: a dataframe object of csv file\n",
    "    Output:\n",
    "        returns an array of numpy arrays    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    \n",
    "    for index, row in items.iterrows():\n",
    "\n",
    "        cats = {\n",
    "                'protest': True,\n",
    "                'violence': True,\n",
    "                'sign': True,\n",
    "                'photo': True,\n",
    "                'fire': True,\n",
    "                'police': True,\n",
    "                'children': True,\n",
    "                'group_20': True,\n",
    "                'group_100': True,\n",
    "                'flag': True,\n",
    "                'night': True,\n",
    "                'shouting': True\n",
    "               }\n",
    "\n",
    "        if row['protest']:\n",
    "            \n",
    "            array = []\n",
    "            \n",
    "            for cat in cats:\n",
    "                f = float(row[cat])\n",
    "                array.append(f)\n",
    "            \n",
    "            narray = np.array(array)\n",
    "        \n",
    "            values.append(narray)\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    \"\"\"\n",
    "    Load the csv file and return an array of numpy arrays of it's objects\n",
    "    \n",
    "    Args:\n",
    "        path: a relative path to load csv file from\n",
    "    Output:\n",
    "        returns an array of the numpy objects\n",
    "    \"\"\"\n",
    "    labels = pd.read_csv(path)\n",
    "    \n",
    "    train_labels = convert_labels_to_numpy_array(labels)  \n",
    "    \n",
    "    return train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(images, size):\n",
    "    \"\"\"\n",
    "    Reshape the images array adding one more dimension as batch size\n",
    "    \n",
    "    Args:\n",
    "        images : list of images\n",
    "        size   : batch size\n",
    "    Output:\n",
    "        returns the numpy array of reshaped objects\n",
    "    \"\"\"\n",
    "    return np.asarray(images).reshape(size,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_model):\n",
    "    \"\"\"\n",
    "    Get the base model and modify it \n",
    "    \n",
    "    Args:\n",
    "        base_model: It is the object of our pretrained model\n",
    "    Output:\n",
    "        returns the new_model by adding new layers and setting first few layers as \n",
    "        non trainable\n",
    "    \"\"\"\n",
    "    bottleneck_input = base_model.get_layer(index=0).input\n",
    "    bottleneck_output = base_model.get_layer(index=-2).output\n",
    "    \n",
    "    bottleneck_model = Model(inputs=bottleneck_input, outputs=bottleneck_output)\n",
    "    \n",
    "    for layer in bottleneck_model.layers:\n",
    "        \n",
    "        layer.trainable = False\n",
    "    \n",
    "    new_model = Sequential()\n",
    "    \n",
    "    new_model.add(bottleneck_model)\n",
    "    \n",
    "    new_model.add(Dense(12))\n",
    "    \n",
    "    new_model.add(Dropout(0.5))\n",
    "    \n",
    "    new_model.add(Dense(12, input_dim=2048, activation='softmax'))\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our base model\n",
    "base_model = InceptionV3()\n",
    "\n",
    "# get the new modified model\n",
    "new_model = get_model(base_model)\n",
    "\n",
    "# compile the model with optimizer, loss function and add a metrics for accuracy\n",
    "new_model.compile(optimizer='rmsprop', loss='mean_absolute_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training labels file path\n",
    "training_labels_path = \"training.csv\"\n",
    "\n",
    "# create a dataframe object by loading that file using pandas\n",
    "df = pd.read_csv(training_labels_path)\n",
    "\n",
    "# store the image names which have protest label set\n",
    "train_image_names = {}\n",
    "\n",
    "# iterate over the file and modify the file name to \"type/filename\". Here type is either train or test.\n",
    "# this will help us in further mapping labels and images with each other\n",
    "for index, item in df.iterrows():\n",
    "    \n",
    "    if item['protest']:\n",
    "        name = \"train/\" + item['fname']\n",
    "        train_image_names[name] = True\n",
    "        \n",
    "# test labels file path\n",
    "test_labels_path = \"test.csv\"\n",
    "\n",
    "# read the test csv file\n",
    "test_df = pd.read_csv(test_labels_path)\n",
    "\n",
    "# store the test image names which have protest label set\n",
    "test_image_names = {}\n",
    "\n",
    "# iterate over the file and modify the file name to \"type/filename\". Here type is either train or test.\n",
    "# this will help us in further mapping labels and images with each other\n",
    "for index, item in test_df.iterrows():\n",
    "    \n",
    "    if item['protest']:\n",
    "        name = \"test/\" + item['fname']\n",
    "        \n",
    "        test_image_names[name] = True\n",
    "\n",
    "# path of training images folder\n",
    "training_images_path = \"train/\"\n",
    "\n",
    "# path of test images folder\n",
    "test_images_path = \"test/\"\n",
    "\n",
    "# get all the training labels\n",
    "train_labels = get_labels(training_labels_path)\n",
    "\n",
    "# get all the training images\n",
    "train_images = get_images(training_images_path, train_image_names, True)\n",
    "\n",
    "# get all the test labels\n",
    "test_labels = get_labels(test_labels_path)\n",
    "\n",
    "# get all the test images\n",
    "test_images = get_images(test_images_path, test_image_names, False)\n",
    "\n",
    "# size of our training images dataset\n",
    "train_size = len(train_images)\n",
    "\n",
    "# size of our test images dataset\n",
    "test_size = len(test_images)\n",
    "\n",
    "# validation size of data\n",
    "validation_size = test_size // 3\n",
    "\n",
    "# reshaped train images\n",
    "reshaped_train_images = reshape_images(train_images, train_size)\n",
    "\n",
    "# reshaped test images\n",
    "reshaped_test_images = reshape_images(test_images[validation_size:test_size], test_size - validation_size)\n",
    "\n",
    "\n",
    "# validation images\n",
    "validation_images = reshape_images(test_images[0:validation_size], validation_size)\n",
    "\n",
    "\n",
    "\n",
    "# validation labels \n",
    "validation_labels = test_labels[0:validation_size]\n",
    "\n",
    "\n",
    "\n",
    "# test labels \n",
    "new_test_labels = test_labels[validation_size:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 768/9316 [=>............................] - ETA: 19:05 - loss: 0.2683 - acc: 0.1862"
     ]
    }
   ],
   "source": [
    "# initialize our base model\n",
    "base_model = InceptionV3()\n",
    "\n",
    "# get the new modified model\n",
    "new_model = get_model(base_model)\n",
    "\n",
    "# compile the model with optimizer, loss function and add a metrics for accuracy\n",
    "new_model.compile(optimizer='rmsprop', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "# fit our training images dataset\n",
    "new_model.fit(reshaped_train_images, np.array(train_labels),epochs=1, shuffle=True)\n",
    "\n",
    "# save the state of our model, so that we won't need to run it again from scratch in future\n",
    "new_model.save('news_image_with_dropout_with_dropout_twelve.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict label of our test images\n",
    "new_model = load_model('news_image_with_dropout_with_dropout_twelve.h5')\n",
    "predict = new_model.predict(reshaped_test_images, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy of our model\n",
    "new_model.evaluate(test_images_dataset, np.array(test_labels_dataset), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels[781:]\n",
    "predictY = predict\n",
    "testY = np.array(test_labels)\n",
    "predicted_violence = []\n",
    "predicted_protest = []\n",
    "predicted_sign = []\n",
    "actual_protest = []\n",
    "actual_violence = []\n",
    "actual_sign = []\n",
    "actual_photo = []\n",
    "predicted_photo = []\n",
    "actual_fire = []\n",
    "predicted_fire = []\n",
    "for i in range(0,5,1):\n",
    "    predicted_protest.append(predictY[i][0])\n",
    "    predicted_violence.append(predictY[i][1])\n",
    "    predicted_sign.append(predictY[i][2])\n",
    "    predicted_photo.append(predictY[i][3])\n",
    "    predicted_fire.append(predictY[i][4])\n",
    "    #print(predicted)\n",
    "    actual_protest.append(testY[i][0])\n",
    "    actual_violence.append(testY[i][1])\n",
    "    actual_sign.append(testY[i][2])\n",
    "    actual_photo.append(testY[i][3])\n",
    "    actual_fire.append(testY[i][4])\n",
    "    #print(actual)\n",
    "\n",
    "    \n",
    "#plt.figure(figsize=(10,4))\n",
    "\n",
    "fig,axs = plt.subplots(5, 1)\n",
    "\n",
    "axs[0].plot(actual_protest, 'r-', label='Actual')\n",
    "axs[0].plot(predicted_protest, 'g-', label='Predicted')\n",
    "axs[0].set_title('Protest')\n",
    "fig.suptitle('Categorical Predicted Values', fontsize=16)\n",
    "\n",
    "\n",
    "axs[1].plot(actual_violence, 'r-', label='Actual')\n",
    "axs[1].plot(predicted_violence, 'g-', label='Predicted')\n",
    "axs[1].set_title('Violence')\n",
    "\n",
    "axs[2].plot(actual_sign, 'r-', label='Actual')\n",
    "axs[2].plot(predicted_sign, 'g-', label='Predicted')\n",
    "axs[2].set_title('Sign')\n",
    "\n",
    "axs[3].plot(actual_photo, 'r-', label='Actual')\n",
    "axs[3].plot(predicted_photo, 'g-', label='Predicted')\n",
    "axs[3].set_title('Photo')\n",
    "\n",
    "axs[4].plot(actual_fire, 'r-', label='Actual')\n",
    "axs[4].plot(predicted_fire, 'g-', label='Predicted')\n",
    "axs[4].set_title('Fire')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
